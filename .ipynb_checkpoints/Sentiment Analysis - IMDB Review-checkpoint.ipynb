{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4363ea37",
   "metadata": {},
   "source": [
    "# <center> IMDB Review Sentiment Analysis <br> Using Tensorflow-Keras Neural Network</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd6bd00",
   "metadata": {},
   "source": [
    "#### <center>Chris Davis <br> October, 2021</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146adf6c",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "\n",
    "- ***[Importing Libraries](#Importing-Libraries)***\n",
    "- ***[Preprocessing Text](#Preprocessing-Text)***\n",
    "- ***[Adding GloVe embedded layer](#Adding-GloVe-embedded-layer)***\n",
    "- ***[Model Building](#Model-BUilding)***\n",
    "- ***[Model Fit and Performance](#Model-Fit-and-Performance)***\n",
    "- ***[Citations](#Citations)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b24711",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c86c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e691a",
   "metadata": {},
   "source": [
    "### Preprocessing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd74c08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Import raw data file\n",
    "#df_raw = pd.read_csv('Amazon Labeled Comments.csv', sep=',')\n",
    "df_raw = pd.read_csv('IMDB Reviews.csv', sep=',')\n",
    "\n",
    "#View raw data\n",
    "df_raw.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea779d",
   "metadata": {},
   "source": [
    "***- Perform text standarization, stopword removal, and lemmatization:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090cd40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regex cleaning function\n",
    "def text_clean(data):\n",
    "    # Convert to lower\n",
    "    data = data.lower()\n",
    "\n",
    "    # Remove new lines\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "\n",
    "    # Convert ?!. to \" \"\n",
    "    data = re.sub('[?!]', '.', data)\n",
    "\n",
    "    # Remove non alpha characters\n",
    "    data = re.sub('[^a-zA-Z\\']', ' ', data)\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    data = re.sub(' +', ' ', data)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Define remove stopwords function\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word for word in text.split() if word not in stop]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Define lemmatization function\n",
    "Ltzr = WordNetLemmatizer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    filtered_words = [Ltzr.lemmatize(word) for word in text.split()]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Create copy of raw data for processing\n",
    "df_clean = df_raw.copy()\n",
    "\n",
    "# Clean/standardize text\n",
    "df_clean.Comment = df_clean.Comment.apply(lambda x: text_clean(x))\n",
    "\n",
    "# Remove stopwords\n",
    "df_clean.Comment = df_clean.Comment.apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# Lemmatize\n",
    "df_clean.Comment = df_clean.Comment.apply(lambda x: word_lemmatizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc0845",
   "metadata": {},
   "source": [
    "***- Define test/train split and tokenize:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5dcfff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split to test/train\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df_clean.Comment,\n",
    "                                                    df_clean.Sentiment,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.2)\n",
    "# Tokenize\n",
    "tokenizer = Tokenizer(oov_token='OOV')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "unique_words = list(tokenizer.word_index.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937d74f",
   "metadata": {},
   "source": [
    "***- Pad sequences:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345291b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View comment length histogram to set max pad length\n",
    "plt.hist([len(i) for i in X_train])\n",
    "plt.xlabel(\"Word Count\")\n",
    "plt.ylabel(\"Occurence Count\")\n",
    "plt.title(\"Comment Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ca5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad Sequences\n",
    "max_length = 300\n",
    "X_train = pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b69f962",
   "metadata": {},
   "source": [
    "### Adding GloVe embedded layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe386f",
   "metadata": {},
   "source": [
    "***- Creating trained GloVe embedded layer:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3452758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create GloVe embedding layer\n",
    "embeddings_index = {}\n",
    "with open(\"Glove Data\\glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b1f34",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc23b7",
   "metadata": {},
   "source": [
    "***- Building Keras sequential model:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c1723a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_words, 100, weights=[embedding_matrix], input_length=max_length, trainable=False))\n",
    "model.add(layers.SpatialDropout1D(0.4))\n",
    "model.add(layers.LSTM(64, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Loss function\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "#optimizer\n",
    "optim = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "#Define metrics\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "#Define early stopping metrics\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience =5, mode = 'max')\n",
    "\n",
    "#Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer=optim, metrics=metrics)\n",
    "\n",
    "#Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87c6f8",
   "metadata": {},
   "source": [
    "### Model Composition:\n",
    "    \n",
    "***Embedded Layer:*** GloVe embedded layer\n",
    "\n",
    "***Dropout Layer:*** Added to prevent over-fitting\n",
    "\n",
    "***LSTM Layer:*** Stacked long short-term memory\n",
    "\n",
    "***Dense Layer:*** Sigmoid activation binary output\n",
    "\n",
    "\n",
    "### Model Parameters:\n",
    "\n",
    "***loss function:***\n",
    "Cross entropy loss function applied with out logistic fitting because the sigmoid function was already applied in the activation function.\n",
    "\n",
    "***optimizer:***\n",
    "'Adam' optimizer used (stochastic gradient descent method).\n",
    "\n",
    "***stopping criteria:***\n",
    "Stopping criteria is evaluated on the accuracy of the test set predictions as this is the metric I am trying to maximize. If there are 10 epochs of decreasing accuracy, then the model will stop fitting prematurely.\n",
    "\n",
    "***evaluation metric:***\n",
    "The model is tuned to maximize prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f7450",
   "metadata": {},
   "source": [
    "### Model Fit and Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a75f7c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model to training data\n",
    "history = model.fit(X_train, Y_train, epochs=50, validation_data=(X_test, Y_test), verbose=2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44db6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return prediction loss and accuracy\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d682818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training visualization\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['val_loss']\n",
    "epochs = range(1, len(val_accuracy) + 1)\n",
    "plt.plot(epochs, val_accuracy)\n",
    "plt.plot(epochs, loss)\n",
    "plt.title('Validation Performance')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel('Accuracy/Loss')\n",
    "plt.legend(['test_accuracy', 'test_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train accuracy vs validation accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Train vs Validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863645a9",
   "metadata": {},
   "source": [
    "The tuned model correctly predicts review sentiment in the test set with an accuracy of %. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f738c",
   "metadata": {},
   "source": [
    "### Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1fc7f0",
   "metadata": {},
   "source": [
    "Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation\n",
    "\n",
    "Oliinyk, Halyna (2017). https://towardsdatascience.com/word-embeddings-exploration-explanation-and-exploitation-with-code-in-python-5dac99d5d795.(\"Word embeddings: exploration, explanation, and exploitation (with code in Python)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_NLP]",
   "language": "python",
   "name": "conda-env-env_NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
